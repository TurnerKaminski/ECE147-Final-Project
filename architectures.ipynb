{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import the models and functions\n",
    "import torch\n",
    "import os.path\n",
    "from models import *\n",
    "from trainer import *\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load in data, this assumed you have a folder in env named data\n",
    "data_dir = \"data\"\n",
    "X_test = np.load(os.path.join(data_dir, \"X_test.npy\"))\n",
    "y_test = np.load(os.path.join(data_dir, \"y_test.npy\"))\n",
    "person_test = np.load(os.path.join(data_dir, \"person_test.npy\")).squeeze(axis=1)\n",
    "X_train_valid = np.load(os.path.join(data_dir, \"X_train_valid.npy\"))\n",
    "y_train_valid = np.load(os.path.join(data_dir, \"y_train_valid.npy\"))\n",
    "person_train_valid = np.load(os.path.join(data_dir, \"person_train_valid.npy\")).squeeze(axis=1)\n",
    "\n",
    "# Predefine some useful variables and fix data a bit\n",
    "n_class = len(set(y_train_valid))\n",
    "n_trials = 5\n",
    "min_y = min(y_train_valid)\n",
    "y_train_valid = y_train_valid - min_y\n",
    "y_test = y_test - min_y\n",
    "\n",
    "# Set up data sets\n",
    "s1_indices_train = [i for i, s in enumerate(person_train_valid) if s == 0]\n",
    "s1_indices_test = [i for i, s in enumerate(person_test) if s == 0]\n",
    "X_train_valid_s1 = X_train_valid[s1_indices_train,:,:400]\n",
    "y_train_valid_s1 = y_train_valid[s1_indices_train]\n",
    "X_test_s1 = X_test[s1_indices_test,:,:400]\n",
    "y_test_s1 = y_test[s1_indices_test]\n",
    "\n",
    "# Make dataloader test set for the single subject\n",
    "# Convert data to tensors\n",
    "X_tensor = torch.FloatTensor(X_test_s1)\n",
    "y_tensor = torch.LongTensor(y_test_s1)\n",
    "\n",
    "# Combine X and y into a TensorDataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Prepare dataloaders\n",
    "test_dataloader_s1 = DataLoader(dataset, batch_size=256, shuffle=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Tonmoy architecture\n",
    "Tonmoy_summary = HybridCNNLSTM()\n",
    "summary(Tonmoy_summary, input_size=(64, 22, 1000))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/turnerkaminski/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "HybridCNNLSTM                            [64, 4]                   --\n",
       "├─Sequential: 1-1                        [64, 25, 18, 332]         --\n",
       "│    └─Conv2d: 2-1                       [64, 25, 18, 996]         650\n",
       "│    └─ELU: 2-2                          [64, 25, 18, 996]         --\n",
       "│    └─MaxPool2d: 2-3                    [64, 25, 18, 332]         --\n",
       "│    └─BatchNorm2d: 2-4                  [64, 25, 18, 332]         50\n",
       "│    └─Dropout: 2-5                      [64, 25, 18, 332]         --\n",
       "├─Sequential: 1-2                        [64, 50, 14, 109]         --\n",
       "│    └─Conv2d: 2-6                       [64, 50, 14, 328]         31,300\n",
       "│    └─ELU: 2-7                          [64, 50, 14, 328]         --\n",
       "│    └─MaxPool2d: 2-8                    [64, 50, 14, 109]         --\n",
       "│    └─BatchNorm2d: 2-9                  [64, 50, 14, 109]         100\n",
       "│    └─Dropout: 2-10                     [64, 50, 14, 109]         --\n",
       "├─Sequential: 1-3                        [64, 100, 10, 35]         --\n",
       "│    └─Conv2d: 2-11                      [64, 100, 10, 105]        125,100\n",
       "│    └─ELU: 2-12                         [64, 100, 10, 105]        --\n",
       "│    └─MaxPool2d: 2-13                   [64, 100, 10, 35]         --\n",
       "│    └─BatchNorm2d: 2-14                 [64, 100, 10, 35]         200\n",
       "│    └─Dropout: 2-15                     [64, 100, 10, 35]         --\n",
       "├─Sequential: 1-4                        [64, 200, 6, 10]          --\n",
       "│    └─Conv2d: 2-16                      [64, 200, 6, 31]          500,200\n",
       "│    └─ELU: 2-17                         [64, 200, 6, 31]          --\n",
       "│    └─MaxPool2d: 2-18                   [64, 200, 6, 10]          --\n",
       "│    └─BatchNorm2d: 2-19                 [64, 200, 6, 10]          400\n",
       "│    └─Dropout: 2-20                     [64, 200, 6, 10]          --\n",
       "├─Sequential: 1-5                        [64, 10]                  --\n",
       "│    └─Linear: 2-21                      [64, 40]                  480,040\n",
       "│    └─ReLU: 2-22                        [64, 40]                  --\n",
       "│    └─Dropout: 2-23                     [64, 40]                  --\n",
       "│    └─Linear: 2-24                      [64, 10]                  410\n",
       "├─LSTM: 1-6                              [64, 1, 40]               8,320\n",
       "├─Linear: 1-7                            [64, 4]                   164\n",
       "├─Softmax: 1-8                           [64, 4]                   --\n",
       "==========================================================================================\n",
       "Total params: 1,146,934\n",
       "Trainable params: 1,146,934\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 24.34\n",
       "==========================================================================================\n",
       "Input size (MB): 5.63\n",
       "Forward/backward pass size (MB): 559.51\n",
       "Params size (MB): 4.59\n",
       "Estimated Total Size (MB): 569.73\n",
       "=========================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "# EEGNet architecture\n",
    "EEGnet_summary = CNN(input_size=X_train_valid.shape[1:], N=n_class)\n",
    "summary(EEGnet_summary, input_size=(64, 22, 1000))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN                                      [64, 4]                   --\n",
       "├─Sequential: 1-1                        [64, 8, 22, 969]          --\n",
       "│    └─Conv2d: 2-1                       [64, 8, 22, 969]          256\n",
       "│    └─BatchNorm2d: 2-2                  [64, 8, 22, 969]          16\n",
       "├─Sequential: 1-2                        [64, 16, 1, 969]          --\n",
       "│    └─Conv2d: 2-3                       [64, 16, 1, 969]          352\n",
       "│    └─BatchNorm2d: 2-4                  [64, 16, 1, 969]          32\n",
       "│    └─ELU: 2-5                          [64, 16, 1, 969]          --\n",
       "├─AvgPool2d: 1-3                         [64, 16, 1, 484]          --\n",
       "├─Dropout: 1-4                           [64, 16, 1, 484]          --\n",
       "├─Sequential: 1-5                        [64, 16, 1, 119]          --\n",
       "│    └─Conv2d: 2-6                       [64, 16, 1, 477]          2,048\n",
       "│    └─Conv2d: 2-7                       [64, 16, 1, 477]          256\n",
       "│    └─BatchNorm2d: 2-8                  [64, 16, 1, 477]          32\n",
       "│    └─ELU: 2-9                          [64, 16, 1, 477]          --\n",
       "│    └─AvgPool2d: 2-10                   [64, 16, 1, 119]          --\n",
       "│    └─Dropout: 2-11                     [64, 16, 1, 119]          --\n",
       "├─Linear: 1-6                            [64, 4]                   7,620\n",
       "==========================================================================================\n",
       "Total params: 10,612\n",
       "Trainable params: 10,612\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 441.93\n",
       "==========================================================================================\n",
       "Input size (MB): 5.63\n",
       "Forward/backward pass size (MB): 202.24\n",
       "Params size (MB): 0.04\n",
       "Estimated Total Size (MB): 207.91\n",
       "=========================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\n",
    "# LSTM architecture\n",
    "lstm_summary = LSTM()\n",
    "summary(lstm_summary, input_size=(64, 22, 1000))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LSTM                                     [64, 4]                   --\n",
       "├─LSTM: 1-1                              [64, 1000, 64]            89,088\n",
       "├─Sequential: 1-2                        [64, 4]                   --\n",
       "│    └─Linear: 2-1                       [64, 54]                  3,510\n",
       "│    └─BatchNorm1d: 2-2                  [64, 54]                  108\n",
       "│    └─ReLU: 2-3                         [64, 54]                  --\n",
       "│    └─Dropout: 2-4                      [64, 54]                  --\n",
       "│    └─Linear: 2-5                       [64, 44]                  2,420\n",
       "│    └─BatchNorm1d: 2-6                  [64, 44]                  88\n",
       "│    └─ReLU: 2-7                         [64, 44]                  --\n",
       "│    └─Linear: 2-8                       [64, 24]                  1,080\n",
       "│    └─BatchNorm1d: 2-9                  [64, 24]                  48\n",
       "│    └─ReLU: 2-10                        [64, 24]                  --\n",
       "│    └─Linear: 2-11                      [64, 4]                   100\n",
       "==========================================================================================\n",
       "Total params: 96,442\n",
       "Trainable params: 96,442\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 5.70\n",
       "==========================================================================================\n",
       "Input size (MB): 5.63\n",
       "Forward/backward pass size (MB): 32.89\n",
       "Params size (MB): 0.39\n",
       "Estimated Total Size (MB): 38.91\n",
       "=========================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# LSTM architecture\n",
    "mini_summary = small_CRNN()\n",
    "summary(mini_summary, input_size=(64, 22, 1000))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/turnerkaminski/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Sequential: 1, Conv2d: 2, ReLU: 2, BatchNorm2d: 2, Dropout: 2, Sequential: 1, Conv2d: 2, ReLU: 2, BatchNorm2d: 2, Dropout: 2]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torchinfo/torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 295\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1529\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1579\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1579\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/Desktop/GitHub/ECE147-Final-Project/models.py:317\u001b[0m, in \u001b[0;36msmall_CRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# FC layer\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# Reshape for LSTM\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1529\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1579\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1579\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1529\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1579\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1579\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x63744 and 1600x128)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# LSTM architecture\u001b[39;00m\n\u001b[1;32m      2\u001b[0m mini_summary \u001b[38;5;241m=\u001b[39m small_CRNN()\n\u001b[0;32m----> 3\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmini_summary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m22\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torchinfo/torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m validate_user_params(\n\u001b[1;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    218\u001b[0m )\n\u001b[1;32m    220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[1;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[1;32m    222\u001b[0m )\n\u001b[0;32m--> 223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[1;32m    227\u001b[0m results \u001b[38;5;241m=\u001b[39m ModelStatistics(\n\u001b[1;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[1;32m    229\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torchinfo/torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    303\u001b[0m     executed_layers \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mexecuted]\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Sequential: 1, Conv2d: 2, ReLU: 2, BatchNorm2d: 2, Dropout: 2, Sequential: 1, Conv2d: 2, ReLU: 2, BatchNorm2d: 2, Dropout: 2]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.13 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "cc54b9e43af7a0c7fb4d06dbdd44e5ca6fb86b76a909759481753182cba9a8da"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}